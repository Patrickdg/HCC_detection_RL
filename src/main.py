#LIBRARIES
import math
import os
import pandas as pd
import tqdm
from sklearn.model_selection import train_test_split

import src.policies as p
import src.classes as c
import src.regression as r

#SETUP
POLICY_MAPPING = {
    'random': p.policy_random, 
    'myopic': p.policy_myopic,
    'egreedy': p.policy_egreedy,
    'interval': p.policy_interval_estimation, 
    'boltzmann': p.policy_boltzmann,
    'deep': c.PolicyDL
}

#FUNCTIONS
def dir_checks():
    dirs = ['data', 'results']
    for d in dirs: 
        if not os.path.exists(d):
            os.mkdir(d)

def save_results(results_df, params): 
    try: 
        df = pd.read_csv('results/results.csv')
    except: 
        df = pd.DataFrame()
        df.to_csv('results/results.csv', index=False)
    #Drop any existing rows with identical unique parameter run ID 
    if len(df) > 0: 
        drop_mask = df['id'] == params['id']
        df = df.drop(df[drop_mask].index)

    df = pd.concat([df, results_df])
    df.to_csv('results/results.csv', index=False)

def main(data_path, T=3650, D=90, num_iters=400, n=500, k=0.2, policy='random', policy_params={}, replace=True, d=0.1, store_results=False):
    """
    ======================== SIMULATION PARAMETERS ========================
    @data_path: path to subjects.csv (to be generated by 'src/generate_data.py' script)
    @T: planning horizon for decision maker (DM) (days) 
    @D: decision epochs period (days)
    @num_iters: iterations to repeat simulation & policy run
    @n: panel size at any given time, subset of 'N' total subjects in pool
    @k: screening resource constraint level as a proportion of total number of subjects in panel 'n' 
    @policy: policy function for selection of panel subjects to be screened at decision epochs
        Available policies == ['random', 'myopic', 'egreedy', 'interval', 'boltzmann']
    @policy_params: params custom for each policy. Each policy func receives [panel, policy_params, k, coefs] args. 
        Required params per policy function: 
            - random:               {}
            - myopic:               {}
            - e-greedy:             {'e':   float in [0,1]}
            - interval estimation:  {'z':   int   in [0,inf)}
            - boltzmann:            {'tau': float in (0,inf)}
            - deep:                 {}
    @replace: boolean flag to determine if subjects moved from pool N to panel n should be with replacement
        - if False, then must ensure that total pool has enough subjects N to sustain sampling throughout horizon
    @d: proportion of panel to sample for random exit of simulation 
    """

    """
    ======================== SIMULATION EVENT LOGIC ========================
    0) Initialize
        - Metrics E, L, X
        - Time period t = 0
        - Agent decision maker (DM)
        - Train/test split of full subject pool --> train Logistic regression model to obtain risk factor coefficients.
    1) Initial Panel Module:: initialize panel with size 'n' for decision maker from total pool 'N' 
        - Identify between C: cancer patients and NC: non-cancer patients 
    2) Policy Module:: input each patient's states to DM, choose subset k of patients to screen based on policy
        - For non-selected patients, DM's knowledge of them remains unchanged until the next period t+1
        - For selected patients, increment X += 1 for screenings spent on cancer patients. Then, subjects enter Imaging Module
    3) Imaging Module:: determine each screened patient's cancer state: 
        a) if cancer-free, output cancer-free state. 
        b) if cancer, determine current cancer stage (early/late/cancer-free) based on random sampling from tumor size (s) distribution: 
            1 <= s <= 5 --> early-stage
            5 <  s      --> late-stage
            Otherwise   --> cancer-free
        c) Updates: 
            - if cancer-free, AFP Reading Module:: assign new AFP reading for this patient, add to DM's knowledge of panel
            - if early- or late-stage, increment E (or L) += 1, then replace patient in New Patient Module:: draw new patient from pool with probability based on follow up times
    4) Patient Exit Module:: randomly sample 'd' proportion of panel to leave study. 
    5) Repeat until end of planning horizon T
    6) Report metrics: 
        - E/(E+L): proportion of cancers detected in early stage
        - X/(K*T): proportion of resources spent on patients who eventually develop cancer
    """
    #0) Initialize metrics & data
    dir_checks()
    policy_func = POLICY_MAPPING[policy]
    decision_epochs = math.floor(T/D)
    pool = pd.read_csv(data_path)

    sim_metrics = pd.DataFrame()
    for n_iter in tqdm.tqdm(range(num_iters)):
        #0) Initialize time period 't', decision maker (DM) agent, train/test split for log_reg
        t = 0
        train_pool, test_pool = train_test_split(pool, test_size=0.5)
        log_model = r.train_model(train_pool)
        if policy == 'deep': 
            policy_func_ = policy_func({'batch': 32, 'memory_size': math.floor(n*0.4)})
            agent = c.Agent_DL(policy_func_, policy_params, k, n, decision_epochs, log_model)
        else: 
            agent = c.Agent(policy_func, policy_params, k, n, decision_epochs, log_model)

        #1) Initial Panel Module:: samples pool, and determines subsets C: cancer, NC: no-cancer
        pool_remaining = agent.init_panel(test_pool, replace)

        for t_ in range(decision_epochs): 
            # 2) Policy Module:: input each patient's states to DM, choose subset k of patients to screen based on policy
            k_subjects = agent.get_k_subjects()
            # 3) Imaging Module:: determine cancer state; cancer-free -> update AFP readings, early-/late-stage -> run New Patient Module:: 
            k_subjects = agent.screen(k_subjects)
            k_subjects, pool_remaining = agent.update_screened(k_subjects, pool_remaining, replace)
            # 4) Patient Exit Module:: randomly sample 'd' proportion of panel to leave study. 
            pool_remaining = agent.patient_exit_module(pool_remaining, d=d)
            t += 1
        iter_metrics = agent.report_metrics(n_iter)
        sim_metrics = pd.concat([sim_metrics, iter_metrics]).round(4)

    #Store metric results
    means = pd.DataFrame(sim_metrics.mean())
    means.index = [str(i)+'_mean' for i in means.index]
    stds = pd.DataFrame(sim_metrics.std())
    stds.index = [str(i)+'_std' for i in stds.index]
    sim_metrics = pd.concat([means, stds], axis=0)
    if store_results:
        params = {
                'decision_epochs': decision_epochs, 
                'num_iters': num_iters, 
                'n': n, 
                'k': k, 
                'policy': f"'{str(policy)}'",
                'policy_params': f'{str(policy_params)}',
                'replace': replace,
                'd': d}
        unique_run_id = ''.join([str(v) for k, v in params.items()])
        params['id'] = unique_run_id
        results = pd.concat([pd.Series(params), sim_metrics], axis=0)
        save_results(pd.DataFrame(results).T, params)
    return sim_metrics

def run_full_sim():
    k_levels = [0.1, 0.2, 0.3, 0.4]
    run_policies = [
        ['random', {}],
        ['myopic', {}],
        ['egreedy', {'e': [0.05, 0.1, 0.25]}],
        ['interval', {'z': [1, 3, 5]}],
        ['boltzmann', {'tau': [0.250, 0.325, 0.400]}]
        ['deep', {}]
        ]

    for k in k_levels: 
        for param in run_policies: 
            policy, policy_params = param
            if not policy_params: 
                p_name = ''
                p_vals = ['']
            else: 
                p_name = list(policy_params.keys())[0]
                p_vals = policy_params[p_name]
            for p_val in p_vals: 
                print(f"==== RUNNING k == {k} {policy}, {p_name} == {p_val} ====")
                p_val = {} if policy in ['random', 'myopic'] else {p_name: p_val}
                df = main(
                        'data/subjects.csv', 
                        T=3650, 
                        D=90, 
                        num_iters=50, 
                        n=500, 
                        k=k, 
                        policy=policy, 
                        policy_params=p_val,
                        replace=True, 
                        d=0.05, 
                        store_results=True
                    )

#RUN
if __name__ == "__main__":
    run_full_sim()
